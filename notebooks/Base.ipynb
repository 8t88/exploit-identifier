{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: a sequence of tokens, and a token_to_index dictionary\n",
    "# output: a LongTensor variable to encode the sequence of idxs\n",
    "def prepare_sequence(seq, to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([to_ix[w] for w in seq.split(' ')]))\n",
    "    return var\n",
    "\n",
    "def prepare_label(label,label_to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([label_to_ix[label]]))\n",
    "    return var\n",
    "\n",
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent.split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix\n",
    "\n",
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "files = ['../data/VDISC_test.hdf5', '../data/VDISC_train.hdf5', '../data/VDISC_validate.hdf5']\n",
    "\n",
    "sample_data_train = []\n",
    "sample_data_test = []\n",
    "with h5py.File(files[0], 'r') as f:\n",
    "    for x in range(0,200):\n",
    "        label = 0\n",
    "        if (f['CWE-120'][x]):\n",
    "            label = 1 #change to casting from bool later\n",
    "        sample_data_train.append((f['functionSource'][x], label))\n",
    "    for x in range(200,300):\n",
    "        label = 0\n",
    "        if (f['CWE-120'][x]):\n",
    "            label = 1 #change to casting from bool later\n",
    "        sample_data_test.append((f['functionSource'][x], label)) \n",
    "\n",
    "print(len(sample_data_train))\n",
    "print(len(sample_data_test))\n",
    "\n",
    "\n",
    "\n",
    "word_to_ix = build_token_to_ix([s for s,_ in sample_data_train + sample_data_test])\n",
    "label_to_ix = {0:0,1:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this seems to be a common starter class\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, loss_function, word_to_ix, label_to_ix, name ='dev'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "\n",
    "    for sent, label in data:\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ' avg_loss:%g train acc:%g' % (avg_loss, acc ))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth)==len(pred)\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i]==pred[i]:\n",
    "            right += 1.0\n",
    "    return right/len(truth)\n",
    "\n",
    "def evaluate(model, data, loss_function, word_to_ix, label_to_ix, name ='dev'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "\n",
    "    for sent, label in data:\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ' avg_loss:%g train acc:%g' % (avg_loss, acc ))\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    batch_sent = []\n",
    "\n",
    "    for sent, label in train_data:\n",
    "\n",
    "\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print('epoch: %d iterations: %d loss :%g' % (i, count, loss.data[0]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_data)\n",
    "    print('epoch: %d done! \\n train avg_loss:%g , acc:%g'%(i, avg_loss, get_accuracy(truth_res,pred_res)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/train/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/train/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3cba50c66520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: %d start!'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'now best dev acc:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_dev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#    dev_acc = evaluate(model,dev_data,loss_function,word_to_ix,label_to_ix,'dev')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e1a04ff1ce48>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "EPOCH = 2#100\n",
    "BATCH_SIZE = 10\n",
    "weight_decay = 1e-4\n",
    "learning_rate = 0.01\n",
    "\n",
    "lstm_model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=HIDDEN_DIM, vocab_size=len(word_to_ix),label_size=len(label_to_ix))\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "loss_func = nn.NLLLoss(size_average=False)\n",
    "\n",
    "#training part\n",
    "for i in range(EPOCH):\n",
    "    random.shuffle(sample_data_train)\n",
    "    print('epoch: %d start!' % i)\n",
    "    train_epoch(lstm_model, sample_data_train, loss_func, optimizer, word_to_ix, label_to_ix, i)\n",
    "    print('now best dev acc:',best_dev_acc)\n",
    "#    dev_acc = evaluate(model,dev_data,loss_function,word_to_ix,label_to_ix,'dev')\n",
    "    test_acc = evaluate(lstm_model, sample_data_test, loss_func, word_to_ix, label_to_ix, 'test')\n",
    "#     if dev_acc > best_dev_acc:\n",
    "#         best_dev_acc = dev_acc\n",
    "#         os.system('rm mr_best_model_acc_*.model')\n",
    "#         print('New Best Dev!!!')\n",
    "#         torch.save(model.state_dict(), 'best_models/mr_best_model_acc_' + str(int(test_acc*10000)) + '.model')\n",
    "#         no_up = 0\n",
    "#     else:\n",
    "#         no_up += 1\n",
    "#         if no_up >= 10:\n",
    "#             exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
